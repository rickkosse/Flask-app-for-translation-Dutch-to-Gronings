{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# NMT-Keras tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Decoding with a trained Neural Machine Translation Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll load from disk a trained Neural Machine Translation (NMT) model. We'll apply it for translating new text. In this case, we want to translate the 'test' split of our dataset.\n",
    "\n",
    "This tutorial assumes that you followed both previous tutorials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, let's import some stuff and load the dataset instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[30/11/2016 17:16:59] <<< Loading Dataset instance from datasets/Dataset_tutorial_dataset.pkl ... >>>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[30/11/2016 17:16:59] <<< Dataset instance loaded >>>\n"
     ]
    }
   ],
   "source": [
    "from config import load_parameters\n",
    "from data_engine.prepare_data import keep_n_captions\n",
    "from keras_wrapper.cnn_model import loadModel\n",
    "from keras_wrapper.dataset import loadDataset\n",
    "params = load_parameters()\n",
    "dataset = loadDataset('datasets/Dataset_tutorial_dataset.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to translate a new data split ('test') we must add it to the dataset instance, just as we did before (at the first tutorial). In case we also had the refences of the test split and we wanted to evaluate it, we can add it to the dataset. Note that this is not mandatory and we could just predict without evaluating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[30/11/2016 17:17:04] Loaded \"test\" set inputs of type \"text\" with id \"source_text\" and length 2996.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[30/11/2016 17:17:04] Loaded \"test\" set inputs of type \"ghost\" with id \"state_below\" and length 2996.\n"
     ]
    }
   ],
   "source": [
    "dataset.setInput('examples/EuTrans/test.es',\n",
    "            'test',\n",
    "            type='text',\n",
    "            id='source_text',\n",
    "            pad_on_batch=True,\n",
    "            tokenization='tokenize_none',\n",
    "            fill='end',\n",
    "            max_text_len=30,\n",
    "            min_occ=0)\n",
    "\n",
    "dataset.setInput(None,\n",
    "            'test',\n",
    "            type='ghost',\n",
    "            id='state_below',\n",
    "            required=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's load the translation model. Suppose we want to load the model saved at the end of the epoch 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[30/11/2016 17:17:07] <<< Loading model from trained_models/tutorial_model/epoch_4_Model_Wrapper.pkl ... >>>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[30/11/2016 17:17:10] <<< Model loaded in 2.7996 seconds. >>>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[30/11/2016 17:17:10] Preparing optimizer and compiling.\n"
     ]
    }
   ],
   "source": [
    "params['INPUT_VOCABULARY_SIZE'] = dataset.vocabulary_len[params['INPUTS_IDS_DATASET'][0]]\n",
    "params['OUTPUT_VOCABULARY_SIZE'] = dataset.vocabulary_len[params['OUTPUTS_IDS_DATASET'][0]]\n",
    "\n",
    "# Load model\n",
    "nmt_model = loadModel('trained_models/tutorial_model', 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we loaded the model, we just have to invoke the sampling method (in this case, the Beam Search algorithm) for the 'test' split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[30/11/2016 17:17:21] <<< Predicting outputs of test set >>>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\rSampling 1/2996  -  ETA: -1s "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\rSampling 2/2996  -  ETA: 19932s "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\rSampling 2995/2996  -  ETA: 0s "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\rSampling 2996/2996  -  ETA: 0s "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total cost of the translations: 159.292059 \t Average cost of the translations: 0.053168\nThe sampling took: 682.227991 secs (Speed: 0.227713 sec/sample)\n"
     ]
    }
   ],
   "source": [
    "params_prediction = {'max_batch_size': 50,\n",
    "                     'n_parallel_loaders': 8,\n",
    "                     'predict_on_sets': ['test'],\n",
    "                     'beam_size': 12,\n",
    "                     'maxlen': 50,\n",
    "                     'model_inputs': ['source_text', 'state_below'],\n",
    "                     'model_outputs': ['target_text'],\n",
    "                     'dataset_inputs': ['source_text', 'state_below'],\n",
    "                     'dataset_outputs': ['target_text'],\n",
    "                     'normalize': True,\n",
    "                     'alpha_factor': 0.6                   \n",
    "                     }\n",
    "predictions = nmt_model.predictBeamSearchNet(dataset, params_prediction)['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to this moment, in the variable 'predictions', we have the indices of the words of the hypotheses. We must decode them into words. For doing this, we'll use the dictionary stored in the dataset object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[30/11/2016 17:28:57] Decoding beam search prediction ...\n"
     ]
    }
   ],
   "source": [
    "from keras_wrapper.utils import decode_predictions_beam_search\n",
    "vocab = dataset.vocabulary['target_text']['idx2words']\n",
    "predictions = decode_predictions_beam_search(predictions,\n",
    "                                             vocab,\n",
    "                                             verbose=params['VERBOSE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we store the system hypotheses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = nmt_model.model_path+'/' + 'test' + '_sampling.pred'  # results file\n",
    "from keras_wrapper.extra.read_write import list2file\n",
    "list2file(filepath, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have the references of this split, we can also evaluate the performance of our system on it. First, we must add them to the dataset object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[30/11/2016 17:29:42] Loaded \"test\" set outputs of type \"text\" with id \"target_text\" and length 2996.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[30/11/2016 17:29:42] Keeping 1 captions per input on the test set.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[30/11/2016 17:29:43] Samples reduced to 2996 in test set.\n"
     ]
    }
   ],
   "source": [
    "# In case we had the references of this split, we could also load the split and evaluate on it\n",
    "dataset.setOutput('examples/EuTrans/test.en',\n",
    "             'test',\n",
    "             type='text',\n",
    "             id='target_text',\n",
    "             pad_on_batch=True,\n",
    "             tokenization='tokenize_none',\n",
    "             sample_weights=True,\n",
    "             max_text_len=30,\n",
    "             max_words=0)\n",
    "keep_n_captions(dataset, repeat=1, n=1, set_names=['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we call the evaluation system: The COCO package. Although its main usage is for multimodal captioning, we can use it in machine translation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[30/11/2016 17:33:48] Computing coco scores on the test split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[30/11/2016 17:33:48] Bleu_1: 0.991317697799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[30/11/2016 17:33:48] Bleu_2: 0.987540341905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[30/11/2016 17:33:48] Bleu_3: 0.983835279345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[30/11/2016 17:33:48] Bleu_4: 0.980146481838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[30/11/2016 17:33:48] CIDEr: 9.70615051823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[30/11/2016 17:33:48] ROUGE_L: 0.990315105909\n"
     ]
    }
   ],
   "source": [
    "from keras_wrapper.extra.evaluation import select\n",
    "metric = 'coco'\n",
    "# Apply sampling\n",
    "extra_vars = dict()\n",
    "extra_vars['tokenize_f'] = eval('dataset.' + 'tokenize_none')\n",
    "extra_vars['language'] = params['TRG_LAN']\n",    
    "extra_vars['test'] = dict()\n",
    "extra_vars['test']['references'] = dataset.extra_variables['test']['target_text']\n",
    "metrics = select[metric](pred_list=predictions,\n",
    "                                          verbose=1,\n",
    "                                          extra_vars=extra_vars,\n",
    "                                          split='test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
